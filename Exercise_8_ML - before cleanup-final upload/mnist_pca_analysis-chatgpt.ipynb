{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43afc975",
   "metadata": {},
   "source": [
    "# MNIST PCA Analysis\n",
    "\n",
    "This notebook performs Principal Component Analysis (PCA) on a CSV file found in the current directory. It creates: \n",
    "\n",
    "- A bar plot of variance explained by each principal component.\n",
    "- A combined bar + cumulative variance plot.\n",
    "- A grid of scatter plots visualizing relationships between pairs of principal components (PC1 vs PC2, PC3 vs PC4, ...).\n",
    "\n",
    "The notebook automatically detects the first `.csv` file in the current directory. If you want to target a specific filename, edit the `csv_file` variable in the first code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d1d394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and CSV detection\n",
    "import os, glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Find the first CSV file in the current directory\n",
    "csv_files = sorted(glob.glob(\"*.csv\"))\n",
    "if len(csv_files) == 0:\n",
    "    raise FileNotFoundError(\"No CSV files found in current directory. Please place your MNIST CSV file here.\")\n",
    "\n",
    "csv_file = csv_files[0]\n",
    "print(f\"Using CSV file: {csv_file}\")\n",
    "\n",
    "# Quick peek - do not load entire file yet if huge\n",
    "df = pd.read_csv(csv_file)\n",
    "print('DataFrame shape:', df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca73ccc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data: assume each row is an image flattened to 784 columns (or more generally numeric pixels)\n",
    "# If there's a 'label' column we drop it for this unsupervised PCA.\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# If a label column exists but named 'label' or 'Label', drop it automatically.\n",
    "possible_label_cols = [c for c in numeric_cols if c.lower() in ('label', 'labels', 'target', 'y')]\n",
    "if possible_label_cols:\n",
    "    print('Dropping possible label columns:', possible_label_cols)\n",
    "    numeric_cols = [c for c in numeric_cols if c not in possible_label_cols]\n",
    "\n",
    "X = df[numeric_cols].values.astype(float)\n",
    "print('Feature matrix shape:', X.shape)\n",
    "\n",
    "# Normalize pixel values if range suggests image pixels (0-255)\n",
    "if X.max() > 1.0:\n",
    "    X = X / 255.0\n",
    "    print('Normalized X by dividing by 255.')\n",
    "\n",
    "# If flattened but user wants image-form, they can reshape later:\n",
    "# images = X.reshape(-1, 28, 28)  # if appropriate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044342d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform PCA\n",
    "n_components = min(100, X.shape[1])  # compute up to 100 PCs or number of features, whichever smaller\n",
    "pca = PCA(n_components=n_components, svd_solver='randomized', random_state=42)\n",
    "pca.fit(X)\n",
    "\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "print('Explained variance shape:', explained_variance.shape)\n",
    "\n",
    "# Bar plot of explained variance per PC\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.bar(range(1, len(explained_variance)+1), explained_variance)\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.title('Explained Variance Ratio by Principal Component')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a77305f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar plot + Cumulative variance curve\n",
    "cum_variance = np.cumsum(explained_variance)\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(12,6))\n",
    "ax1.bar(range(1, len(explained_variance)+1), explained_variance, label='Explained Variance')\n",
    "ax1.set_xlabel('Principal Component')\n",
    "ax1.set_ylabel('Explained Variance Ratio')\n",
    "ax1.set_title('Explained Variance and Cumulative Explained Variance')\n",
    "ax1.set_ylim(0, max(explained_variance)*1.2)\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(range(1, len(cum_variance)+1), cum_variance, marker='o', linestyle='-', label='Cumulative Variance')\n",
    "ax2.set_ylabel('Cumulative Explained Variance Ratio')\n",
    "ax2.set_ylim(0,1.05)\n",
    "\n",
    "# Legends\n",
    "lines, labels = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax1.legend(lines + lines2, labels + labels2, loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print number of components needed for common thresholds\n",
    "for thr in [0.5, 0.75, 0.90, 0.95, 0.99]:\n",
    "    k = np.searchsorted(cum_variance, thr) + 1\n",
    "    print(f\"Number of components to explain {int(thr*100)}% variance: {k}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61fd169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plots of PC pairs (PC1 vs PC2, PC3 vs PC4, ...)\n",
    "# Project data to first few PCs for visualization\n",
    "num_plot_pcs = min(8, X.shape[1])  # up to first 8 PCs\n",
    "X_pca = pca.transform(X)[:, :num_plot_pcs]\n",
    "\n",
    "# Determine grid size (pairs)\n",
    "pairs = []\n",
    "for i in range(0, num_plot_pcs, 2):\n",
    "    if i+1 < num_plot_pcs:\n",
    "        pairs.append((i, i+1))\n",
    "\n",
    "n_pairs = len(pairs)\n",
    "cols = 2\n",
    "rows = (n_pairs + cols - 1) // cols\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(6*cols, 4*rows))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (i, j) in enumerate(pairs):\n",
    "    ax = axes[idx]\n",
    "    ax.scatter(X_pca[:, i], X_pca[:, j], s=8, alpha=0.6)\n",
    "    ax.set_xlabel(f'PC{i+1}')\n",
    "    ax.set_ylabel(f'PC{j+1}')\n",
    "    ax.set_title(f'PC{i+1} vs PC{j+1}')\n",
    "    \n",
    "# Turn off any unused subplots\n",
    "for k in range(len(pairs), len(axes)):\n",
    "    fig.delaxes(axes[k])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# If you want to color points by a label (if available), provide label array 'y' and use ax.scatter(..., c=y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9c27a7",
   "metadata": {},
   "source": [
    "## Next steps & Tips\n",
    "\n",
    "- If you have labels for a subset of the dataset, color the scatter plots by label to see which clusters correspond to which digits.\n",
    "- Consider using **t-SNE** or **UMAP** for improved 2D visualization of complex manifolds.\n",
    "- For better clustering, train an **autoencoder** and cluster the latent space.\n",
    "- To save the PCA model and reuse it: use `joblib.dump(pca, 'pca_model.joblib')`.\n",
    "\n",
    "If you'd like, I can modify this notebook to target a specific filename (instead of picking the first CSV), or add t-SNE/UMAP and clustering cells automatically.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
